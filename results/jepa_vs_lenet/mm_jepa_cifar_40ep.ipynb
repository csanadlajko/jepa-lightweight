{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb9925fe",
      "metadata": {
        "id": "bb9925fe"
      },
      "source": [
        "## Colab file for running the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "de0eee39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de0eee39",
        "outputId": "019e5591-d207-4a4e-db18-ff19ddb99589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'jepa-lightweight'...\n",
            "remote: Enumerating objects: 549, done.\u001b[K\n",
            "remote: Counting objects: 100% (284/284), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 549 (delta 119), reused 224 (delta 72), pack-reused 265 (from 1)\u001b[K\n",
            "Receiving objects: 100% (549/549), 7.90 MiB | 24.53 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n",
            "/content/jepa-lightweight\n"
          ]
        }
      ],
      "source": [
        "# mount drive if needed (eg. dataset stored in drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "## /drive/MyDrive when using drive\n",
        "%cd /content\n",
        "!rm -r jepa-lightweight\n",
        "!git clone https://github.com/csanadlajko/jepa-lightweight.git\n",
        "## /drive/MyDrive/jepa-lightweight when using drive\n",
        "%cd /content/jepa-lightweight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f9eb9ec6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9eb9ec6",
        "outputId": "a60abe91-36bc-4424-d9a8-b9e44ee6f3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Collecting torchviz (from -r requirements.txt (line 2))\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.24.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.57.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz->-r requirements.txt (line 2)) (0.21)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (2025.11.12)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n",
            "100% 170M/170M [00:14<00:00, 12.2MB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 5.97MB/s]\n",
            "2026-01-11 17:49:15.142573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 17:49:15.160148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768153755.181200    2616 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768153755.187727    2616 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768153755.204688    2616 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768153755.204717    2616 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768153755.204720    2616 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768153755.204722    2616 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 17:49:15.211200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 54.2MB/s]\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 4.10MB/s]\n",
            "vocab.txt: 232kB [00:00, 62.7MB/s]\n",
            "tokenizer.json: 466kB [00:00, 95.3MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.36MB/s]\n",
            "total number of parameters approx.: 32434244\n",
            "Which from are trainable parameters: 32434244\n",
            "\n",
            "Starting training in MULTIMODAL mode for 40 epoch(s)!\n",
            "\n",
            "=== EPOCH 1/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2247\n",
            "\n",
            "=== EPOCH 2/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2026\n",
            "\n",
            "=== EPOCH 3/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2050\n",
            "\n",
            "=== EPOCH 4/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2138\n",
            "\n",
            "=== EPOCH 5/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2200\n",
            "\n",
            "=== EPOCH 6/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2275\n",
            "\n",
            "=== EPOCH 7/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2349\n",
            "\n",
            "=== EPOCH 8/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2427\n",
            "\n",
            "=== EPOCH 9/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2485\n",
            "\n",
            "=== EPOCH 10/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2516\n",
            "\n",
            "=== EPOCH 11/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2545\n",
            "\n",
            "=== EPOCH 12/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2574\n",
            "\n",
            "=== EPOCH 13/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2589\n",
            "\n",
            "=== EPOCH 14/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2586\n",
            "\n",
            "=== EPOCH 15/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2629\n",
            "\n",
            "=== EPOCH 16/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2666\n",
            "\n",
            "=== EPOCH 17/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2662\n",
            "\n",
            "=== EPOCH 18/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2701\n",
            "\n",
            "=== EPOCH 19/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2703\n",
            "\n",
            "=== EPOCH 20/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2672\n",
            "\n",
            "=== EPOCH 21/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2703\n",
            "\n",
            "=== EPOCH 22/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2721\n",
            "\n",
            "=== EPOCH 23/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2772\n",
            "\n",
            "=== EPOCH 24/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2803\n",
            "\n",
            "=== EPOCH 25/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2832\n",
            "\n",
            "=== EPOCH 26/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2823\n",
            "\n",
            "=== EPOCH 27/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2834\n",
            "\n",
            "=== EPOCH 28/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2824\n",
            "\n",
            "=== EPOCH 29/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2846\n",
            "\n",
            "=== EPOCH 30/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2843\n",
            "\n",
            "=== EPOCH 31/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2875\n",
            "\n",
            "=== EPOCH 32/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2850\n",
            "\n",
            "=== EPOCH 33/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2826\n",
            "\n",
            "=== EPOCH 34/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2795\n",
            "\n",
            "=== EPOCH 35/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2774\n",
            "\n",
            "=== EPOCH 36/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2735\n",
            "\n",
            "=== EPOCH 37/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2717\n",
            "\n",
            "=== EPOCH 38/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2650\n",
            "\n",
            "=== EPOCH 39/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2649\n",
            "\n",
            "=== EPOCH 40/40 ===\n",
            "---STARTING EPOCH---\n",
            "---EPOCH ENDED---\n",
            "Average training loss: 0.2636\n",
            "\n",
            "=== CLS EPOCH 1/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 2.5662, Accuracy: 0.0625\n",
            "CLS Loss at batch 500: 1.3542, Accuracy: 0.2899\n",
            "CLS Loss at batch 1000: 1.5552, Accuracy: 0.3269\n",
            "CLS Loss at batch 1500: 1.8222, Accuracy: 0.3474\n",
            "CLS Loss at batch 2000: 1.8768, Accuracy: 0.3619\n",
            "CLS Loss at batch 2500: 1.7846, Accuracy: 0.3748\n",
            "CLS Loss at batch 3000: 1.1776, Accuracy: 0.3867\n",
            "Average CLS training loss: 1.6808\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 2/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.7763, Accuracy: 0.3750\n",
            "CLS Loss at batch 500: 1.4621, Accuracy: 0.4528\n",
            "CLS Loss at batch 1000: 1.4490, Accuracy: 0.4542\n",
            "CLS Loss at batch 1500: 1.0296, Accuracy: 0.4567\n",
            "CLS Loss at batch 2000: 1.4344, Accuracy: 0.4634\n",
            "CLS Loss at batch 2500: 1.4248, Accuracy: 0.4659\n",
            "CLS Loss at batch 3000: 1.2229, Accuracy: 0.4691\n",
            "Average CLS training loss: 1.4747\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 3/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.6999, Accuracy: 0.3750\n",
            "CLS Loss at batch 500: 1.3426, Accuracy: 0.4976\n",
            "CLS Loss at batch 1000: 1.5077, Accuracy: 0.4991\n",
            "CLS Loss at batch 1500: 1.6044, Accuracy: 0.4991\n",
            "CLS Loss at batch 2000: 1.4489, Accuracy: 0.4999\n",
            "CLS Loss at batch 2500: 1.4020, Accuracy: 0.5041\n",
            "CLS Loss at batch 3000: 1.7042, Accuracy: 0.5052\n",
            "Average CLS training loss: 1.3826\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 4/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.2832, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 1.3538, Accuracy: 0.5205\n",
            "CLS Loss at batch 1000: 1.5582, Accuracy: 0.5190\n",
            "CLS Loss at batch 1500: 1.8392, Accuracy: 0.5206\n",
            "CLS Loss at batch 2000: 1.6322, Accuracy: 0.5210\n",
            "CLS Loss at batch 2500: 1.4251, Accuracy: 0.5246\n",
            "CLS Loss at batch 3000: 1.2718, Accuracy: 0.5259\n",
            "Average CLS training loss: 1.3206\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 5/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.3040, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 1.4621, Accuracy: 0.5447\n",
            "CLS Loss at batch 1000: 1.6027, Accuracy: 0.5452\n",
            "CLS Loss at batch 1500: 1.0569, Accuracy: 0.5472\n",
            "CLS Loss at batch 2000: 1.1392, Accuracy: 0.5465\n",
            "CLS Loss at batch 2500: 1.5269, Accuracy: 0.5470\n",
            "CLS Loss at batch 3000: 1.6773, Accuracy: 0.5464\n",
            "Average CLS training loss: 1.2789\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 6/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.3241, Accuracy: 0.5000\n",
            "CLS Loss at batch 500: 1.4004, Accuracy: 0.5603\n",
            "CLS Loss at batch 1000: 1.3660, Accuracy: 0.5521\n",
            "CLS Loss at batch 1500: 1.2147, Accuracy: 0.5537\n",
            "CLS Loss at batch 2000: 1.4129, Accuracy: 0.5533\n",
            "CLS Loss at batch 2500: 1.0315, Accuracy: 0.5554\n",
            "CLS Loss at batch 3000: 1.1839, Accuracy: 0.5560\n",
            "Average CLS training loss: 1.2444\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 7/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.6331, Accuracy: 0.8125\n",
            "CLS Loss at batch 500: 0.9510, Accuracy: 0.5711\n",
            "CLS Loss at batch 1000: 0.9849, Accuracy: 0.5664\n",
            "CLS Loss at batch 1500: 1.9076, Accuracy: 0.5693\n",
            "CLS Loss at batch 2000: 1.3805, Accuracy: 0.5672\n",
            "CLS Loss at batch 2500: 1.0280, Accuracy: 0.5653\n",
            "CLS Loss at batch 3000: 1.6918, Accuracy: 0.5660\n",
            "Average CLS training loss: 1.2158\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 8/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.1460, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 1.4727, Accuracy: 0.5796\n",
            "CLS Loss at batch 1000: 0.9620, Accuracy: 0.5814\n",
            "CLS Loss at batch 1500: 1.4069, Accuracy: 0.5767\n",
            "CLS Loss at batch 2000: 0.7131, Accuracy: 0.5751\n",
            "CLS Loss at batch 2500: 1.0061, Accuracy: 0.5767\n",
            "CLS Loss at batch 3000: 1.3237, Accuracy: 0.5780\n",
            "Average CLS training loss: 1.1889\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 9/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9251, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 1.1042, Accuracy: 0.5928\n",
            "CLS Loss at batch 1000: 1.2800, Accuracy: 0.5872\n",
            "CLS Loss at batch 1500: 0.8282, Accuracy: 0.5862\n",
            "CLS Loss at batch 2000: 0.9317, Accuracy: 0.5856\n",
            "CLS Loss at batch 2500: 0.7516, Accuracy: 0.5852\n",
            "CLS Loss at batch 3000: 1.1152, Accuracy: 0.5848\n",
            "Average CLS training loss: 1.1703\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 10/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.2314, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 1.2984, Accuracy: 0.5992\n",
            "CLS Loss at batch 1000: 1.1505, Accuracy: 0.5982\n",
            "CLS Loss at batch 1500: 0.6931, Accuracy: 0.5967\n",
            "CLS Loss at batch 2000: 0.9361, Accuracy: 0.5949\n",
            "CLS Loss at batch 2500: 1.5903, Accuracy: 0.5944\n",
            "CLS Loss at batch 3000: 1.4674, Accuracy: 0.5951\n",
            "Average CLS training loss: 1.1454\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 11/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.8714, Accuracy: 0.7500\n",
            "CLS Loss at batch 500: 1.2348, Accuracy: 0.5952\n",
            "CLS Loss at batch 1000: 1.5963, Accuracy: 0.5952\n",
            "CLS Loss at batch 1500: 1.2189, Accuracy: 0.5976\n",
            "CLS Loss at batch 2000: 1.0126, Accuracy: 0.5968\n",
            "CLS Loss at batch 2500: 1.4387, Accuracy: 0.5960\n",
            "CLS Loss at batch 3000: 0.9117, Accuracy: 0.5965\n",
            "Average CLS training loss: 1.1342\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 12/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.7247, Accuracy: 0.6875\n",
            "CLS Loss at batch 500: 0.8796, Accuracy: 0.6073\n",
            "CLS Loss at batch 1000: 0.7843, Accuracy: 0.6050\n",
            "CLS Loss at batch 1500: 1.3831, Accuracy: 0.6045\n",
            "CLS Loss at batch 2000: 1.2293, Accuracy: 0.6058\n",
            "CLS Loss at batch 2500: 1.3635, Accuracy: 0.6048\n",
            "CLS Loss at batch 3000: 0.8968, Accuracy: 0.6048\n",
            "Average CLS training loss: 1.1123\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 13/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.7784, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 1.0934, Accuracy: 0.6079\n",
            "CLS Loss at batch 1000: 1.2038, Accuracy: 0.6085\n",
            "CLS Loss at batch 1500: 1.3076, Accuracy: 0.6112\n",
            "CLS Loss at batch 2000: 0.9447, Accuracy: 0.6125\n",
            "CLS Loss at batch 2500: 0.6982, Accuracy: 0.6119\n",
            "CLS Loss at batch 3000: 1.8616, Accuracy: 0.6126\n",
            "Average CLS training loss: 1.0967\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 14/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.5693, Accuracy: 0.5000\n",
            "CLS Loss at batch 500: 0.5796, Accuracy: 0.6229\n",
            "CLS Loss at batch 1000: 1.2005, Accuracy: 0.6193\n",
            "CLS Loss at batch 1500: 1.0577, Accuracy: 0.6163\n",
            "CLS Loss at batch 2000: 1.7119, Accuracy: 0.6160\n",
            "CLS Loss at batch 2500: 1.4323, Accuracy: 0.6145\n",
            "CLS Loss at batch 3000: 1.2871, Accuracy: 0.6146\n",
            "Average CLS training loss: 1.0943\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 15/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.0591, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 1.3240, Accuracy: 0.6309\n",
            "CLS Loss at batch 1000: 1.0138, Accuracy: 0.6180\n",
            "CLS Loss at batch 1500: 0.6016, Accuracy: 0.6167\n",
            "CLS Loss at batch 2000: 0.7982, Accuracy: 0.6149\n",
            "CLS Loss at batch 2500: 1.0072, Accuracy: 0.6175\n",
            "CLS Loss at batch 3000: 1.3014, Accuracy: 0.6187\n",
            "Average CLS training loss: 1.0777\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 16/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.7425, Accuracy: 0.8125\n",
            "CLS Loss at batch 500: 0.8774, Accuracy: 0.6309\n",
            "CLS Loss at batch 1000: 1.1317, Accuracy: 0.6279\n",
            "CLS Loss at batch 1500: 1.0815, Accuracy: 0.6287\n",
            "CLS Loss at batch 2000: 0.7201, Accuracy: 0.6307\n",
            "CLS Loss at batch 2500: 0.8149, Accuracy: 0.6294\n",
            "CLS Loss at batch 3000: 0.6829, Accuracy: 0.6298\n",
            "Average CLS training loss: 1.0559\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 17/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.7376, Accuracy: 0.8125\n",
            "CLS Loss at batch 500: 0.5505, Accuracy: 0.6349\n",
            "CLS Loss at batch 1000: 0.9154, Accuracy: 0.6375\n",
            "CLS Loss at batch 1500: 0.6065, Accuracy: 0.6345\n",
            "CLS Loss at batch 2000: 0.2864, Accuracy: 0.6349\n",
            "CLS Loss at batch 2500: 0.7347, Accuracy: 0.6347\n",
            "CLS Loss at batch 3000: 1.3518, Accuracy: 0.6333\n",
            "Average CLS training loss: 1.0400\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 18/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.8782, Accuracy: 0.7500\n",
            "CLS Loss at batch 500: 0.8519, Accuracy: 0.6325\n",
            "CLS Loss at batch 1000: 0.9059, Accuracy: 0.6296\n",
            "CLS Loss at batch 1500: 0.6072, Accuracy: 0.6328\n",
            "CLS Loss at batch 2000: 0.8875, Accuracy: 0.6319\n",
            "CLS Loss at batch 2500: 1.0402, Accuracy: 0.6332\n",
            "CLS Loss at batch 3000: 0.7931, Accuracy: 0.6333\n",
            "Average CLS training loss: 1.0377\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 19/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9646, Accuracy: 0.7500\n",
            "CLS Loss at batch 500: 0.5873, Accuracy: 0.6430\n",
            "CLS Loss at batch 1000: 0.4833, Accuracy: 0.6399\n",
            "CLS Loss at batch 1500: 1.0980, Accuracy: 0.6368\n",
            "CLS Loss at batch 2000: 1.4291, Accuracy: 0.6370\n",
            "CLS Loss at batch 2500: 0.6688, Accuracy: 0.6360\n",
            "CLS Loss at batch 3000: 1.2587, Accuracy: 0.6383\n",
            "Average CLS training loss: 1.0269\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 20/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9004, Accuracy: 0.6875\n",
            "CLS Loss at batch 500: 1.0895, Accuracy: 0.6472\n",
            "CLS Loss at batch 1000: 1.5341, Accuracy: 0.6468\n",
            "CLS Loss at batch 1500: 1.1370, Accuracy: 0.6446\n",
            "CLS Loss at batch 2000: 1.5043, Accuracy: 0.6428\n",
            "CLS Loss at batch 2500: 0.5332, Accuracy: 0.6428\n",
            "CLS Loss at batch 3000: 0.7780, Accuracy: 0.6421\n",
            "Average CLS training loss: 1.0158\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 21/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.0178, Accuracy: 0.7500\n",
            "CLS Loss at batch 500: 1.4015, Accuracy: 0.6487\n",
            "CLS Loss at batch 1000: 1.2069, Accuracy: 0.6449\n",
            "CLS Loss at batch 1500: 1.1363, Accuracy: 0.6457\n",
            "CLS Loss at batch 2000: 1.4028, Accuracy: 0.6461\n",
            "CLS Loss at batch 2500: 0.5394, Accuracy: 0.6443\n",
            "CLS Loss at batch 3000: 1.3549, Accuracy: 0.6449\n",
            "Average CLS training loss: 1.0097\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 22/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9623, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 0.5144, Accuracy: 0.6486\n",
            "CLS Loss at batch 1000: 0.6111, Accuracy: 0.6451\n",
            "CLS Loss at batch 1500: 0.9437, Accuracy: 0.6492\n",
            "CLS Loss at batch 2000: 1.1310, Accuracy: 0.6504\n",
            "CLS Loss at batch 2500: 0.7543, Accuracy: 0.6517\n",
            "CLS Loss at batch 3000: 0.7907, Accuracy: 0.6504\n",
            "Average CLS training loss: 0.9949\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 23/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9204, Accuracy: 0.6875\n",
            "CLS Loss at batch 500: 0.7868, Accuracy: 0.6574\n",
            "CLS Loss at batch 1000: 1.1756, Accuracy: 0.6523\n",
            "CLS Loss at batch 1500: 1.0049, Accuracy: 0.6528\n",
            "CLS Loss at batch 2000: 0.7055, Accuracy: 0.6538\n",
            "CLS Loss at batch 2500: 1.2150, Accuracy: 0.6518\n",
            "CLS Loss at batch 3000: 1.0861, Accuracy: 0.6522\n",
            "Average CLS training loss: 0.9862\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 24/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.6460, Accuracy: 0.7500\n",
            "CLS Loss at batch 500: 0.5312, Accuracy: 0.6607\n",
            "CLS Loss at batch 1000: 0.8146, Accuracy: 0.6573\n",
            "CLS Loss at batch 1500: 1.4239, Accuracy: 0.6545\n",
            "CLS Loss at batch 2000: 0.7615, Accuracy: 0.6528\n",
            "CLS Loss at batch 2500: 0.7584, Accuracy: 0.6527\n",
            "CLS Loss at batch 3000: 0.5162, Accuracy: 0.6523\n",
            "Average CLS training loss: 0.9852\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 25/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.5666, Accuracy: 0.9375\n",
            "CLS Loss at batch 500: 0.6831, Accuracy: 0.6627\n",
            "CLS Loss at batch 1000: 1.7477, Accuracy: 0.6628\n",
            "CLS Loss at batch 1500: 0.9497, Accuracy: 0.6611\n",
            "CLS Loss at batch 2000: 1.0777, Accuracy: 0.6602\n",
            "CLS Loss at batch 2500: 0.6934, Accuracy: 0.6580\n",
            "CLS Loss at batch 3000: 1.0372, Accuracy: 0.6569\n",
            "Average CLS training loss: 0.9704\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 26/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.1919, Accuracy: 0.6875\n",
            "CLS Loss at batch 500: 0.6332, Accuracy: 0.6633\n",
            "CLS Loss at batch 1000: 0.9312, Accuracy: 0.6628\n",
            "CLS Loss at batch 1500: 0.6243, Accuracy: 0.6635\n",
            "CLS Loss at batch 2000: 0.9450, Accuracy: 0.6638\n",
            "CLS Loss at batch 2500: 1.0258, Accuracy: 0.6638\n",
            "CLS Loss at batch 3000: 0.8869, Accuracy: 0.6640\n",
            "Average CLS training loss: 0.9614\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 27/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.6005, Accuracy: 0.8125\n",
            "CLS Loss at batch 500: 0.6777, Accuracy: 0.6677\n",
            "CLS Loss at batch 1000: 0.7519, Accuracy: 0.6625\n",
            "CLS Loss at batch 1500: 0.9599, Accuracy: 0.6616\n",
            "CLS Loss at batch 2000: 1.1863, Accuracy: 0.6631\n",
            "CLS Loss at batch 2500: 1.2996, Accuracy: 0.6630\n",
            "CLS Loss at batch 3000: 0.9137, Accuracy: 0.6632\n",
            "Average CLS training loss: 0.9536\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 28/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.8550, Accuracy: 0.7500\n",
            "CLS Loss at batch 500: 1.3186, Accuracy: 0.6669\n",
            "CLS Loss at batch 1000: 0.5489, Accuracy: 0.6663\n",
            "CLS Loss at batch 1500: 0.7650, Accuracy: 0.6679\n",
            "CLS Loss at batch 2000: 0.9370, Accuracy: 0.6693\n",
            "CLS Loss at batch 2500: 1.5342, Accuracy: 0.6679\n",
            "CLS Loss at batch 3000: 1.4704, Accuracy: 0.6669\n",
            "Average CLS training loss: 0.9491\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 29/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.8603, Accuracy: 0.8125\n",
            "CLS Loss at batch 500: 0.9398, Accuracy: 0.6647\n",
            "CLS Loss at batch 1000: 0.8106, Accuracy: 0.6635\n",
            "CLS Loss at batch 1500: 0.9934, Accuracy: 0.6647\n",
            "CLS Loss at batch 2000: 1.4328, Accuracy: 0.6698\n",
            "CLS Loss at batch 2500: 0.9071, Accuracy: 0.6679\n",
            "CLS Loss at batch 3000: 0.9998, Accuracy: 0.6676\n",
            "Average CLS training loss: 0.9413\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 30/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9637, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 0.8038, Accuracy: 0.6583\n",
            "CLS Loss at batch 1000: 0.7950, Accuracy: 0.6643\n",
            "CLS Loss at batch 1500: 0.4028, Accuracy: 0.6671\n",
            "CLS Loss at batch 2000: 0.9775, Accuracy: 0.6675\n",
            "CLS Loss at batch 2500: 1.0628, Accuracy: 0.6699\n",
            "CLS Loss at batch 3000: 0.9525, Accuracy: 0.6699\n",
            "Average CLS training loss: 0.9390\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 31/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.3288, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 1.0035, Accuracy: 0.6750\n",
            "CLS Loss at batch 1000: 0.7628, Accuracy: 0.6716\n",
            "CLS Loss at batch 1500: 0.6492, Accuracy: 0.6721\n",
            "CLS Loss at batch 2000: 0.7365, Accuracy: 0.6727\n",
            "CLS Loss at batch 2500: 1.1596, Accuracy: 0.6725\n",
            "CLS Loss at batch 3000: 0.7447, Accuracy: 0.6725\n",
            "Average CLS training loss: 0.9279\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 32/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.8602, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 0.6528, Accuracy: 0.6790\n",
            "CLS Loss at batch 1000: 1.4421, Accuracy: 0.6743\n",
            "CLS Loss at batch 1500: 0.9157, Accuracy: 0.6762\n",
            "CLS Loss at batch 2000: 0.9340, Accuracy: 0.6749\n",
            "CLS Loss at batch 2500: 0.7881, Accuracy: 0.6743\n",
            "CLS Loss at batch 3000: 0.9221, Accuracy: 0.6746\n",
            "Average CLS training loss: 0.9284\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 33/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.9693, Accuracy: 0.6875\n",
            "CLS Loss at batch 500: 0.8286, Accuracy: 0.6830\n",
            "CLS Loss at batch 1000: 1.2249, Accuracy: 0.6760\n",
            "CLS Loss at batch 1500: 1.0903, Accuracy: 0.6745\n",
            "CLS Loss at batch 2000: 0.7536, Accuracy: 0.6750\n",
            "CLS Loss at batch 2500: 0.8083, Accuracy: 0.6781\n",
            "CLS Loss at batch 3000: 0.9657, Accuracy: 0.6771\n",
            "Average CLS training loss: 0.9172\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 34/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.5943, Accuracy: 0.4375\n",
            "CLS Loss at batch 500: 0.6932, Accuracy: 0.6834\n",
            "CLS Loss at batch 1000: 1.0490, Accuracy: 0.6774\n",
            "CLS Loss at batch 1500: 0.9919, Accuracy: 0.6805\n",
            "CLS Loss at batch 2000: 1.5905, Accuracy: 0.6817\n",
            "CLS Loss at batch 2500: 0.7813, Accuracy: 0.6829\n",
            "CLS Loss at batch 3000: 1.7162, Accuracy: 0.6819\n",
            "Average CLS training loss: 0.9048\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 35/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.3372, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 0.6314, Accuracy: 0.6895\n",
            "CLS Loss at batch 1000: 1.1640, Accuracy: 0.6889\n",
            "CLS Loss at batch 1500: 1.0540, Accuracy: 0.6848\n",
            "CLS Loss at batch 2000: 0.5560, Accuracy: 0.6816\n",
            "CLS Loss at batch 2500: 0.4247, Accuracy: 0.6806\n",
            "CLS Loss at batch 3000: 0.5762, Accuracy: 0.6804\n",
            "Average CLS training loss: 0.9049\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 36/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.4213, Accuracy: 0.5000\n",
            "CLS Loss at batch 500: 1.0947, Accuracy: 0.6909\n",
            "CLS Loss at batch 1000: 0.5959, Accuracy: 0.6894\n",
            "CLS Loss at batch 1500: 0.8648, Accuracy: 0.6849\n",
            "CLS Loss at batch 2000: 0.7456, Accuracy: 0.6863\n",
            "CLS Loss at batch 2500: 0.6387, Accuracy: 0.6866\n",
            "CLS Loss at batch 3000: 0.8382, Accuracy: 0.6863\n",
            "Average CLS training loss: 0.8987\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 37/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.5314, Accuracy: 0.9375\n",
            "CLS Loss at batch 500: 0.7431, Accuracy: 0.6912\n",
            "CLS Loss at batch 1000: 0.7428, Accuracy: 0.6882\n",
            "CLS Loss at batch 1500: 1.1918, Accuracy: 0.6865\n",
            "CLS Loss at batch 2000: 1.1313, Accuracy: 0.6861\n",
            "CLS Loss at batch 2500: 1.0069, Accuracy: 0.6858\n",
            "CLS Loss at batch 3000: 0.5983, Accuracy: 0.6838\n",
            "Average CLS training loss: 0.8984\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 38/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.7648, Accuracy: 0.5625\n",
            "CLS Loss at batch 500: 1.6484, Accuracy: 0.6873\n",
            "CLS Loss at batch 1000: 0.5449, Accuracy: 0.6849\n",
            "CLS Loss at batch 1500: 0.7493, Accuracy: 0.6862\n",
            "CLS Loss at batch 2000: 0.5216, Accuracy: 0.6850\n",
            "CLS Loss at batch 2500: 0.9059, Accuracy: 0.6861\n",
            "CLS Loss at batch 3000: 0.9461, Accuracy: 0.6866\n",
            "Average CLS training loss: 0.8895\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 39/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 1.0743, Accuracy: 0.6250\n",
            "CLS Loss at batch 500: 1.2156, Accuracy: 0.6955\n",
            "CLS Loss at batch 1000: 1.0259, Accuracy: 0.6947\n",
            "CLS Loss at batch 1500: 1.0138, Accuracy: 0.6910\n",
            "CLS Loss at batch 2000: 0.3591, Accuracy: 0.6901\n",
            "CLS Loss at batch 2500: 0.8934, Accuracy: 0.6922\n",
            "CLS Loss at batch 3000: 0.8645, Accuracy: 0.6913\n",
            "Average CLS training loss: 0.8837\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== CLS EPOCH 40/40 ===\n",
            "---STARTING CLS TRAINING---\n",
            "CLS Loss at batch 0: 0.3448, Accuracy: 0.9375\n",
            "CLS Loss at batch 500: 0.8250, Accuracy: 0.6980\n",
            "CLS Loss at batch 1000: 1.7054, Accuracy: 0.6986\n",
            "CLS Loss at batch 1500: 1.0533, Accuracy: 0.6981\n",
            "CLS Loss at batch 2000: 0.6897, Accuracy: 0.6976\n",
            "CLS Loss at batch 2500: 1.4080, Accuracy: 0.6961\n",
            "CLS Loss at batch 3000: 0.7548, Accuracy: 0.6952\n",
            "Average CLS training loss: 0.8719\n",
            "---CLS TRAINING ENDED---\n",
            "\n",
            "=== FINAL EVALUATION ===\n",
            "---STARTING CLS EVALUATION---\n",
            "Batch 0, Current accuracy: 0.8125\n",
            "Batch 50, Current accuracy: 0.7255\n",
            "Batch 100, Current accuracy: 0.7153\n",
            "Batch 150, Current accuracy: 0.7065\n",
            "Batch 200, Current accuracy: 0.7021\n",
            "Batch 250, Current accuracy: 0.6995\n",
            "Batch 300, Current accuracy: 0.7025\n",
            "Batch 350, Current accuracy: 0.6991\n",
            "Batch 400, Current accuracy: 0.7011\n",
            "Batch 450, Current accuracy: 0.6986\n",
            "Batch 500, Current accuracy: 0.6981\n",
            "Batch 550, Current accuracy: 0.6975\n",
            "Batch 600, Current accuracy: 0.6982\n",
            "---CLS EVALUATION ENDED---\n",
            "Final CLS accuracy: 0.6968\n",
            "Figure(800x500)\n",
            "Figure(800x500)\n",
            "-- CLS token classification accuracy: 0.6968\n"
          ]
        }
      ],
      "source": [
        "# install dependecies and run project (with optional argument parameters)\n",
        "!pip install -r requirements.txt\n",
        "!python ijepa.py --multimodal_run y --debug y --epochs 40 --lr 0.0001 --result_folder content"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
